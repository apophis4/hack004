{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10171,
     "status": "ok",
     "timestamp": 1758011977618,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "0v1HVR2jxICC",
    "outputId": "1f11a750-a8fb-448b-a056-4f78e220dc86"
   },
   "outputs": [],
   "source": [
    "!pip -q install pymupdf faiss-cpu openai tiktoken fastapi uvicorn python-multipart pydantic pyngrok pandas numpy rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1758011977735,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "I6fzCjJCzKFk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#redacted\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 73923,
     "status": "ok",
     "timestamp": 1758012051604,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "rGR2sBnxzMOw",
    "outputId": "09d18237-7b78-4274-bbfe-db198c255cec"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "os.makedirs(\"/content/pdfs\", exist_ok=True)\n",
    "print(\"Select one or more NCERT PDFs...\")\n",
    "uploaded = files.upload()\n",
    "for name, data in uploaded.items():\n",
    "    with open(f\"/content/pdfs/{name}\", \"wb\") as f:\n",
    "        f.write(data)\n",
    "print(\"✅ Uploaded to /content/pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1710,
     "status": "ok",
     "timestamp": 1758012268810,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "DV6ndK0Fzmja",
    "outputId": "33d969fb-7b30-4fda-93e3-c0e7b62f63af"
   },
   "outputs": [],
   "source": [
    "# --- Multi-subject KB builder with robust JSONL writing & validation ---\n",
    "\n",
    "import os, re, json, hashlib\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "\n",
    "KB_DIR       = \"/content/kb\"\n",
    "PDF_DIR      = \"/content/pdfs\"\n",
    "META_JSONL   = f\"{KB_DIR}/meta.jsonl\"\n",
    "META_TMP     = f\"{KB_DIR}/meta.tmp.jsonl\"\n",
    "INDEX_PATH   = f\"{KB_DIR}/index.faiss\"\n",
    "BM25_PATH    = f\"{KB_DIR}/bm25.json\"\n",
    "SUBJECTS_JSON= f\"{KB_DIR}/subjects.json\"\n",
    "TOPICS_JSON = \"/content/kb/topics.json\"\n",
    "PROGRESS_JSON = \"/content/kb/progress.json\"\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "SUBJECT_ALIASES = {\n",
    "    \"science\": [\"science\",\"physics\",\"chemistry\",\"biology\",\"bio\"],\n",
    "    \"mathematics\": [\"math\",\"mathematics\",\"algebra\",\"geometry\",\"trigonometry\",\"statistics\",\"probability\",\"arith\"],\n",
    "    \"english\": [\"english\",\"literature\",\"grammar\",\"poem\",\"prose\"],\n",
    "    \"social science\": [\"social science\",\"history\",\"geography\",\"civics\",\"political science\",\"economics\",\"democratic\",\"resources\"],\n",
    "    \"hindi\": [\"hindi\"],\n",
    "    \"computer science\": [\"computer\",\"informatics\",\"cs\",\"ip\",\"information technology\",\"it\"],\n",
    "    \"biology\": [\"biology\",\"bio\",\"life processes\",\"cell\",\"heredity\",\"evolution\",\"reproduction\"],\n",
    "    \"physics\": [\"physics\",\"electricity\",\"magnetism\",\"motion\",\"force\",\"work\",\"energy\",\"light\",\"sound\",\"wave\"],\n",
    "    \"chemistry\": [\"chemistry\",\"chemical\",\"reaction\",\"matter\",\"atom\",\"mole\",\"periodic\",\"compound\",\"mixture\"]\n",
    "}\n",
    "\n",
    "def _safe_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # strip NULs & normalize weird whitespace; keep \\n\n",
    "    s = s.replace(\"\\x00\", \"\")\n",
    "    s = s.encode(\"utf-8\", \"replace\").decode(\"utf-8\", \"replace\")\n",
    "    # collapse repeated spaces while preserving newlines\n",
    "    s = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", s)\n",
    "    # remove stray control chars (except \\n and \\t)\n",
    "    s = re.sub(r\"[\\x01-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f]\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_grade_subject_from_filename(fname: str):\n",
    "    chapter = os.path.splitext(fname)[0].split(\"_\")[2].lower()\n",
    "    base = os.path.splitext(fname)[0].replace(\"_\",\" \").lower()\n",
    "    m = re.search(r\"(class|grade)\\s*([0-9]{1,2})\", base)\n",
    "    grade = int(m.group(2)) if m else None\n",
    "    subject = None\n",
    "    for subj, keys in SUBJECT_ALIASES.items():\n",
    "        if any(k in base for k in keys):\n",
    "            subject = subj; break\n",
    "    if subject is None:\n",
    "        m2 = re.search(r\"([a-z]+)\", base)\n",
    "        subject = m2.group(1) if m2 else \"general\"\n",
    "    return grade, subject, chapter\n",
    "\n",
    "def iter_pdf_pages(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        text = page.get_text(\"text\") or \"\"\n",
    "        yield pno+1, _safe_text(text)\n",
    "\n",
    "def heading_guess(page_text):\n",
    "    lines = [l.strip() for l in page_text.split(\"\\n\") if l.strip()]\n",
    "    chap, title = \"\", \"\"\n",
    "    for i,l in enumerate(lines[:10]):\n",
    "        if re.match(r\"(?i)^chapter\\s+\\d+\\b\", l):\n",
    "            chap = l\n",
    "            if i+1 < len(lines): title = lines[i+1][:120]\n",
    "            break\n",
    "    if not chap and lines:\n",
    "        title = lines[0][:120]\n",
    "    return chap, title\n",
    "\n",
    "def chunk_page(text, max_chars=1200, overlap=100):\n",
    "    # split on blank lines, rebuild chunks under max_chars\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks, buf = [], \"\"\n",
    "    for p in paras:\n",
    "        if len(buf) + len(p) + 2 <= max_chars:\n",
    "            buf = (buf + \"\\n\\n\" + p).strip()\n",
    "        else:\n",
    "            if buf: chunks.append(buf)\n",
    "            buf = p\n",
    "    if buf: chunks.append(buf)\n",
    "    out = []\n",
    "    for i,c in enumerate(chunks):\n",
    "        prev_tail = chunks[i-1][-overlap:] if i>0 else \"\"\n",
    "        out.append((prev_tail + \"\\n\" + c).strip())\n",
    "    return out\n",
    "\n",
    "def embed_texts(texts):\n",
    "    # OpenAI embeddings; normalized for cosine via IP\n",
    "    B = 512\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), B):\n",
    "        resp = client.embeddings.create(model=\"text-embedding-3-large\", input=texts[i:i+B])\n",
    "        arr = np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "        arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12)\n",
    "        vecs.append(arr.astype(\"float32\"))\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def build_kb(pdf_dir=PDF_DIR):\n",
    "    os.makedirs(KB_DIR, exist_ok=True)\n",
    "\n",
    "    metadatas, texts = [], []\n",
    "    subjects_registry = {}\n",
    "\n",
    "    # Collect chunks + metadata\n",
    "    for fname in sorted(os.listdir(pdf_dir)):\n",
    "        if not fname.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        path = os.path.join(pdf_dir, fname)\n",
    "        book = os.path.splitext(fname)[0]\n",
    "        grade, subject, ch = parse_grade_subject_from_filename(fname)\n",
    "        subjects_registry.setdefault(subject, set()).add(grade)\n",
    "\n",
    "        for page_no, page_text in iter_pdf_pages(path):\n",
    "            chap, sec = heading_guess(page_text)\n",
    "            for c in chunk_page(page_text):\n",
    "                metadatas.append({\n",
    "                    \"book\": book,\n",
    "                    \"grade\": grade,\n",
    "                    \"subject\": subject,\n",
    "                    \"chapter\": ch,\n",
    "                    \"section\": sec,\n",
    "                    \"page\": page_no\n",
    "                })\n",
    "                texts.append(c)\n",
    "\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"No chunks created. Did you upload PDFs?\")\n",
    "\n",
    "    # Embeddings + FAISS\n",
    "    print(f\"Embedding {len(texts)} chunks…\")\n",
    "    X = embed_texts(texts)\n",
    "    dim = X.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(X)\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "\n",
    "\n",
    "    with open(META_TMP, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        for m, t in zip(metadatas, texts):\n",
    "            m2 = {**m, \"text\": t}\n",
    "            f.write(json.dumps(m2, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if os.path.exists(META_JSONL):\n",
    "        os.remove(META_JSONL)\n",
    "    os.rename(META_TMP, META_JSONL)\n",
    "\n",
    "\n",
    "    valid_count = 0\n",
    "    with open(META_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            s = line.strip()\n",
    "            if not s: continue\n",
    "            try:\n",
    "                json.loads(s)\n",
    "                valid_count += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                snippet = s[max(0, e.pos-80):e.pos+80]\n",
    "                raise RuntimeError(f\"Invalid JSON at line {i}: {e.msg} (pos {e.pos}).\\nSnippet: {snippet}\") from e\n",
    "    print(f\"meta.jsonl written & validated ({valid_count} lines).\")\n",
    "\n",
    "\n",
    "    tokenized = [t.lower().split() for t in texts]\n",
    "    bm25 = {\"docs\": texts, \"tokenized\": tokenized}\n",
    "    with open(BM25_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bm25, f)\n",
    "\n",
    "\n",
    "    sr = {k: sorted([g for g in v if g is not None]) for k,v in subjects_registry.items()}\n",
    "    with open(SUBJECTS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sr, f, indent=2)\n",
    "\n",
    "    print(f\"KB ready: {len(texts)} chunks; FAISS+BM25 built.\")\n",
    "    print(\"Detected subjects/grades:\", sr)\n",
    "build_kb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1758012273290,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "_rdoBYkdzvK5",
    "outputId": "c5452d0a-6c2b-4050-92fc-ac3b3a0b1905"
   },
   "outputs": [],
   "source": [
    "import json, os, numpy as np, faiss, math, re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "index = faiss.read_index(\"/content/kb/index.faiss\")\n",
    "meta = []\n",
    "with open(\"/content/kb/meta.jsonl\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        meta.append(json.loads(line))\n",
    "\n",
    "with open(\"/content/kb/bm25.json\", encoding=\"utf-8\") as f:\n",
    "    bm25_raw = json.load(f)\n",
    "bm25 = BM25Okapi([d for d in bm25_raw[\"tokenized\"]])\n",
    "\n",
    "with open(\"/content/kb/subjects.json\", encoding=\"utf-8\") as f:\n",
    "    SUBJECTS = json.load(f)\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    r = client.embeddings.create(model=\"text-embedding-3-large\", input=[q])\n",
    "    v = np.array(r.data[0].embedding, dtype=\"float32\")\n",
    "    v = v / (np.linalg.norm(v) + 1e-12)\n",
    "    return v.reshape(1, -1)\n",
    "\n",
    "def guess_subject(query: str):\n",
    "    q = query.lower()\n",
    "    best, best_score = None, 0\n",
    "    for subj, grades in SUBJECTS.items():\n",
    "        keys = subj.split() + sum(([k] for k,v in locals().get('SUBJECT_ALIASES', {}).items() if k==subj), [])\n",
    "        score = sum(1 for k in keys if k in q)\n",
    "\n",
    "        if subj in (\"science\",\"biology\",\"physics\",\"chemistry\") and any(w in q for w in [\"photosynthesis\",\"cell\",\"energy\",\"acid\",\"base\",\"motion\",\"light\",\"electric\",\"magnet\"]):\n",
    "            score += 1\n",
    "        if subj in (\"mathematics\",) and any(w in q for w in [\"theorem\",\"solve\",\"prove\",\"equation\",\"triangle\",\"probability\",\"mean\",\"median\",\"mode\",\"lcm\",\"hcf\"]):\n",
    "            score += 1\n",
    "        if score > best_score:\n",
    "            best, best_score = subj, score\n",
    "    return best if best_score>0 else None\n",
    "\n",
    "def hybrid_retrieve(question: str, subject: str=None, grade: int=None, k_embed=40, k_bm25=40, top=8):\n",
    "    qv = embed_query(question)\n",
    "    D, I = index.search(qv, k_embed)\n",
    "    cand_embed = [(i, float(D[0][j])) for j,i in enumerate(I[0])]\n",
    "\n",
    "\n",
    "    scores = bm25.get_scores(question.lower().split())\n",
    "    top_bm25_ids = np.argsort(scores)[::-1][:k_bm25]\n",
    "    cand_bm25 = [(int(i), float(scores[i])) for i in top_bm25_ids]\n",
    "\n",
    "\n",
    "    cand_map = {}\n",
    "    for i,s in cand_embed:\n",
    "        cand_map.setdefault(i, {\"embed\":0.0,\"bm25\":0.0})\n",
    "        cand_map[i][\"embed\"] = max(cand_map[i][\"embed\"], s)\n",
    "    for i,s in cand_bm25:\n",
    "        cand_map.setdefault(i, {\"embed\":0.0,\"bm25\":0.0})\n",
    "        cand_map[i][\"bm25\"] = max(cand_map[i][\"bm25\"], s)\n",
    "\n",
    "\n",
    "    if cand_map:\n",
    "        max_bm25 = max(v[\"bm25\"] for v in cand_map.values()) or 1.0\n",
    "    fused = []\n",
    "    for i,sc in cand_map.items():\n",
    "        m = meta[i]\n",
    "        subj_boost = 1.0\n",
    "        if subject and m.get(\"subject\")==subject:\n",
    "            subj_boost *= 1.25\n",
    "        if grade and m.get(\"grade\")==grade:\n",
    "            subj_boost *= 1.15\n",
    "\n",
    "        txt = m[\"text\"].lower()\n",
    "        edu_boost = 1.1 if any(k in txt for k in [\"definition\",\"example\",\"key points\",\"summary\",\"exercise\"]) else 1.0\n",
    "        f_score = 0.65*sc[\"embed\"] + 0.35*(sc[\"bm25\"]/max_bm25)\n",
    "        fused.append((i, f_score*subj_boost*edu_boost))\n",
    "\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    picked = []\n",
    "    seen_pages = set()\n",
    "    for i, s in fused:\n",
    "        m = meta[i]\n",
    "        key = (m[\"book\"], m[\"page\"])\n",
    "        if key in seen_pages:\n",
    "            continue\n",
    "        seen_pages.add(key)\n",
    "        picked.append((m, s))\n",
    "        if len(picked)>=top: break\n",
    "    return picked\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a strict NCERT tutor. Use ONLY the provided context. \"\n",
    "    \"Format your response as:\\n\"\n",
    "    \"1) Short definition/overview\\n\"\n",
    "    \"2) Key points (bulleted)\\n\"\n",
    "    \"3) Example or short derivation (if context contains one)\\n\"\n",
    "    \"4) Final takeaway\\n\"\n",
    "    \"Always include explicit NCERT citations after relevant paragraphs like (Book, Ch X, p.Y).\"\n",
    ")\n",
    "\n",
    "def smart_answer(question: str, contexts_with_scores, dontknow_threshold=0.35):\n",
    "\n",
    "    if not contexts_with_scores:\n",
    "        return \"I don’t know. I couldn’t find this in the NCERT materials you uploaded.\"\n",
    "    avg_score = sum(s for _,s in contexts_with_scores)/len(contexts_with_scores)\n",
    "    ctx_text = \"\\\\n\\\\n---\\\\n\\\\n\".join(\n",
    "        f'[{c.get(\"subject\",\"\")}, {c.get(\"book\",\"\")}, {c.get(\"chapter\",\"\")}, p.{c.get(\"page\",\"\")}]\\\\n{c[\"text\"][:1600]}'\n",
    "        for c,_ in contexts_with_scores\n",
    "    )\n",
    "    if avg_score < dontknow_threshold:\n",
    "        prefix = \"I’m not fully confident this is covered in your NCERT set. Here’s my best effort using the closest matches.\\\\n\\\\n\"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "    user = f\"{prefix}Context:\\\\n{ctx_text}\\\\n\\\\nQuestion: {question}\\\\nAnswer following the required format with citations.\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
    "                  {\"role\":\"user\",\"content\":user}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def smart_answer_ext(question: str, subject: str=None, grade: int=None):\n",
    "    ext_prompt = (\n",
    "        f\"Provide broader, real-world context for this question beyond the NCERT syllabus. Keep it factual and suitable for {grade}-th grade student\"\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":ext_prompt},\n",
    "            {\"role\":\"user\",\"content\":question}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def chat_router(question: str, subject: str=None, grade: int=None):\n",
    "    subj = subject or guess_subject(question)\n",
    "    picks = hybrid_retrieve(question, subject=subj, grade=grade)\n",
    "    answer = smart_answer(question, picks)\n",
    "    additional = smart_answer_ext(question, subject=subj, grade=grade)\n",
    "    cites = [{\"book\":m.get(\"book\"),\"subject\":m.get(\"subject\"),\"chapter\":m.get(\"chapter\"),\"page\":m.get(\"page\")} for m,_ in picks]\n",
    "    return answer, cites, subj, additional\n",
    "\n",
    "print(\"Multi-subject hybrid retrieval ready. Try chat_router('Explain photosynthesis for Class 7')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1758012278425,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "6Ma_VpPDzzQa",
    "outputId": "9aac447b-e89b-4087-fa1b-a290c2a642b2"
   },
   "outputs": [],
   "source": [
    "import json, collections\n",
    "\n",
    "rows = [json.loads(l) for l in open(\"/content/kb/meta.jsonl\", encoding=\"utf-8\")]\n",
    "\n",
    "tree = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(list))))\n",
    "for r in rows:\n",
    "    subj = r.get(\"subject\") or \"general\"\n",
    "    grd = str(r.get(\"grade\") or \"NA\")\n",
    "    book = r.get(\"book\")\n",
    "    chap = r.get(\"chapter\") or \"Chapter\"\n",
    "    sec  = r.get(\"section\") or \"Section\"\n",
    "    tree[subj][grd][book][chap].append((sec, r.get(\"page\",0)))\n",
    "\n",
    "out = []\n",
    "for subj, grades in tree.items():\n",
    "    g_list = []\n",
    "    for grd, books in grades.items():\n",
    "        b_list = []\n",
    "        for book, chaps in books.items():\n",
    "            c_list = []\n",
    "            for chap, entries in chaps.items():\n",
    "                sec_pages = collections.defaultdict(list)\n",
    "                for sec, page in entries:\n",
    "                    sec_pages[sec].append(page)\n",
    "                s_list = []\n",
    "                for sec, pages in sec_pages.items():\n",
    "                    ps = sorted(set(pages))\n",
    "                    pages_str = f\"{ps[0]}-{ps[-1]}\" if ps else \"\"\n",
    "                    s_list.append({\"id\": f\"{book}::{chap}::{sec}\", \"title\": sec, \"pages\": pages_str})\n",
    "                c_list.append({\"id\": f\"{book}::{chap}\", \"title\": chap, \"sections\": s_list})\n",
    "            b_list.append({\"book\": book, \"chapters\": c_list})\n",
    "        g_list.append({\"grade\": grd, \"books\": b_list})\n",
    "    out.append({\"subject\": subj, \"grades\": g_list})\n",
    "\n",
    "with open(TOPICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Topics/Subjects index written to\", TOPICS_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1758012281670,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "9PX1C4Y-z2_V",
    "outputId": "cb90fdb2-2da0-4e9e-9eb0-daadf4ce8384"
   },
   "outputs": [],
   "source": [
    "import math, json\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def estimate_minutes_for_text(txt: str) -> int:\n",
    "    tokens = max(50, len(txt.split()))\n",
    "    return max(6, math.ceil(tokens/40.0) + 2)\n",
    "\n",
    "with open(META_JSONL, encoding=\"utf-8\") as f:\n",
    "    rows = [json.loads(l) for l in f]\n",
    "\n",
    "sections_map = {}\n",
    "for r in rows:\n",
    "    key = (r[\"book\"], r.get(\"chapter\",\"\"), r.get(\"section\",\"\"))\n",
    "    secs = sections_map.setdefault(key, {\"book\": r[\"book\"], \"chapter\": r.get(\"chapter\",\"\"), \"section\": r.get(\"section\",\"\"), \"pages\": set(), \"minutes_est\": 0})\n",
    "    secs[\"pages\"].add(r[\"page\"])\n",
    "    secs[\"minutes_est\"] += estimate_minutes_for_text(r[\"text\"])\n",
    "\n",
    "sections = []\n",
    "for (book, chap, sec), v in sections_map.items():\n",
    "    pages = sorted(v[\"pages\"])\n",
    "    v[\"pages\"] = f\"{pages[0]}-{pages[-1]}\" if pages else \"\"\n",
    "    v[\"id\"] = f\"{book}::{chap}::{sec}\"\n",
    "    sections.append(v)\n",
    "\n",
    "sections = sorted(sections, key=lambda x: (x[\"book\"], x[\"chapter\"], x[\"section\"]))\n",
    "\n",
    "def build_plan(start: str, deadline: str, hours_per_day: float):\n",
    "    start_d = date.fromisoformat(start)\n",
    "    end_d = date.fromisoformat(deadline)\n",
    "    minutes_per_day = int(hours_per_day*60)\n",
    "    days = []\n",
    "    d = start_d\n",
    "    while d <= end_d:\n",
    "        days.append({\"date\": d.isoformat(), \"capacity\": minutes_per_day, \"items\": []})\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "    i = 0\n",
    "    for sec in sections:\n",
    "        remaining = sec[\"minutes_est\"]\n",
    "        while remaining > 0 and i < len(days):\n",
    "            slot = min(remaining, days[i][\"capacity\"])\n",
    "            if slot == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            days[i][\"items\"].append({\n",
    "                \"section_id\": sec[\"id\"],\n",
    "                \"book\": sec[\"book\"],\n",
    "                \"chapter\": sec[\"chapter\"],\n",
    "                \"minutes\": slot\n",
    "            })\n",
    "            days[i][\"capacity\"] -= slot\n",
    "            remaining -= slot\n",
    "    if any(day[\"capacity\"] < minutes_per_day for day in days):\n",
    "        reviews = []\n",
    "        for day in days:\n",
    "            for item in day[\"items\"]:\n",
    "                for gap in (1,3,7):\n",
    "                    try_date = (date.fromisoformat(day[\"date\"]) + timedelta(days=gap)).isoformat()\n",
    "                    reviews.append({\"date\": try_date, \"section_id\": item[\"section_id\"], \"minutes\": max(6, item[\"minutes\"]//6)})\n",
    "    else:\n",
    "        reviews = []\n",
    "\n",
    "    return {\"days\": [{k: (v if k!=\"capacity\" else minutes_per_day - v) for k,v in d.items()} for d in days], \"reviews\": reviews}\n",
    "\n",
    "print(\"✅ Planner ready. Example: build_plan('2025-09-16','2025-10-15', 2.0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1758012284513,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "fEApHaYr6XX_"
   },
   "outputs": [],
   "source": [
    "import os, re, json, math, collections\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from json import JSONDecoder, JSONDecodeError # Import for load_meta_jsonl\n",
    "\n",
    "# --- Robust meta loader ---\n",
    "def load_meta_jsonl(path=\"/content/kb/meta.jsonl\"):\n",
    "    \"\"\"\n",
    "    Loads /content/kb/meta.jsonl robustly:\n",
    "      - Handles proper JSONL (one JSON per line)\n",
    "      - Also handles concatenated JSON without newlines (}{ back-to-back)\n",
    "      - Strips NULs and ignores empty/whitespace segments\n",
    "    \"\"\"\n",
    "    # Placeholder: using the logic from cell tZnxJ19J79y-\n",
    "    dec = JSONDecoder()\n",
    "    objs = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        buf = f.read()\n",
    "\n",
    "    # Remove NULs that may appear from odd PDFs\n",
    "    buf = buf.replace(\"\\x00\", \"\")\n",
    "\n",
    "    # Fast path: try line-by-line first (proper JSONL)\n",
    "    lines = buf.splitlines()\n",
    "    if len(lines) > 1:\n",
    "        for ln, line in enumerate(lines, 1):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            try:\n",
    "                objs.append(json.loads(s))\n",
    "            except JSONDecodeError as e:\n",
    "                print(f\"Warning: Skipping invalid JSON line {ln} in {path}: {e}\")\n",
    "                continue\n",
    "        return objs\n",
    "\n",
    "    # Streaming decode: parse sequential JSON objects in one big string\n",
    "    idx = 0\n",
    "    n = len(buf)\n",
    "    while idx < n:\n",
    "        # skip whitespace between JSONs\n",
    "        while idx < n and buf[idx].isspace():\n",
    "            idx += 1\n",
    "        if idx >= n:\n",
    "            break\n",
    "        try:\n",
    "            obj, end = dec.raw_decode(buf, idx)\n",
    "        except JSONDecodeError as e:\n",
    "            # Try to repair the most common issue: missing newline between }{\n",
    "            # Insert a newline at the nearest }{}{ boundary near the error and continue once.\n",
    "            window = buf[max(0, idx-50):min(n, idx+50)]\n",
    "            if \"}{\".encode().decode() in window:\n",
    "                buf = buf.replace(\"}{\", \"}\\n{\")\n",
    "                # Restart the whole streaming pass once after patch\n",
    "                objs.clear()\n",
    "                idx = 0\n",
    "                n = len(buf)\n",
    "                continue\n",
    "            # If still bad, print warning and try to skip the character\n",
    "            context = buf[max(0, idx-140):min(n, idx+140)]\n",
    "            print(f\"Warning: Skipping invalid JSON segment at pos {idx} in {path}. Context: {context}. Error: {e}\")\n",
    "            idx += 1 # Skip one character and try again\n",
    "            continue # Move to the next iteration\n",
    "\n",
    "\n",
    "        objs.append(obj)\n",
    "        idx = end\n",
    "    return objs\n",
    "\n",
    "\n",
    "META_PATH = \"/content/kb/meta.jsonl\"\n",
    "assert os.path.exists(META_PATH), \"meta.jsonl not found. Build the KB first.\"\n",
    "\n",
    "# Load fresh each time to reflect any new ingest\n",
    "def get_meta_rows():\n",
    "    return load_meta_jsonl(META_PATH)\n",
    "\n",
    "\n",
    "# --- Utilities for topic extraction ---\n",
    "CUE_PATTERNS = [\n",
    "    r\"\\b(key points?|summary|in a nutshell|important|remember|definition|define|note|quick recap)\\b\",\n",
    "    r\"\\b(exercise|questions|mcq|short answer|long answer|try yourself)\\b\",\n",
    "    r\"\\b(activity\\s*\\d*|project|experiment|investigate)\\b\",\n",
    "    r\"\\b(objectives?|learning outcomes?)\\b\",\n",
    "    r\"\\b(fascinating facts?|holistic lens|know a scientist)\\b\",\n",
    "]\n",
    "CUE_RE = re.compile(\"|\".join(CUE_PATTERNS), re.I)\n",
    "\n",
    "HEADING_LINE_RE = re.compile(r\"^\\s*(?:\\d+(?:\\.\\d+){0,3})\\s*[:\\-\\)]?\\s*([A-Z][^\\n]{3,200})$\")\n",
    "\n",
    "# broad stopwords; keep domain-neutral\n",
    "GENERIC_STOP = set(\"\"\"chapter exercise question questions answer answers points summary topic topics figure table example examples\n",
    "activity project experiment objectives outcome outcomes learning page pages section subsection student students teacher teachers\n",
    "colour color paper group let us observe discuss try find out make list see also aim material apparatus method conclusion\n",
    "\"\"\".split())\n",
    "\n",
    "def compress_ranges(pages: List[int]) -> str:\n",
    "    if not pages: return \"\"\n",
    "    pages = sorted(set(pages))\n",
    "    rng=[]; s=prev=pages[0]\n",
    "    for p in pages[1:]:\n",
    "        if p==prev+1: prev=p\n",
    "        else: rng.append((s,prev)); s=prev=p\n",
    "    rng.append((s,prev))\n",
    "    return \",\".join([f\"{a}-{b}\" if a!=b else f\"{a}\" for a,b in rng])\n",
    "\n",
    "def extract_chapter_and_book(rows, subject, grade, book, chapter):\n",
    "    ch_blobs, ch_pages = [], []\n",
    "    book_blobs = []\n",
    "    for r in rows:\n",
    "        if r.get(\"subject\")==subject and r.get(\"grade\")==grade:\n",
    "            book_blobs.append(r.get(\"text\",\"\"))\n",
    "            if (r.get(\"chapter\") or \"\") == chapter:\n",
    "                ch_blobs.append(r.get(\"text\",\"\"))\n",
    "                ch_pages.append(r.get(\"page\",0))\n",
    "    return ch_blobs, ch_pages, book_blobs\n",
    "\n",
    "def split_lines(text):\n",
    "    return [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "def cue_heading_boosts(blobs: List[str]):\n",
    "    term_boost = collections.Counter()\n",
    "    page_hits = collections.defaultdict(set)\n",
    "\n",
    "    def norm_phrase(s):\n",
    "        s = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", s.lower())\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    for bi, t in enumerate(blobs):\n",
    "        for ln in split_lines(t):\n",
    "            m = HEADING_LINE_RE.match(ln)\n",
    "            if m:\n",
    "                phrase = norm_phrase(m.group(1))\n",
    "                if phrase and phrase not in GENERIC_STOP and not phrase.isdigit():\n",
    "                    term_boost[phrase] += 2.0\n",
    "                    page_hits[phrase].add(bi)\n",
    "                continue\n",
    "            if CUE_RE.search(ln):\n",
    "                phrase = norm_phrase(ln)\n",
    "                term_boost[phrase] += 1.0\n",
    "                page_hits[phrase].add(bi)\n",
    "\n",
    "        # Capitalized multiword spans\n",
    "        for m in re.finditer(r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,6})\", t):\n",
    "            phrase = norm_phrase(m.group(0))\n",
    "            if phrase and len(phrase.split())>=2:\n",
    "                term_boost[phrase] += 0.6\n",
    "                page_hits[phrase].add(bi)\n",
    "\n",
    "    return term_boost, page_hits\n",
    "\n",
    "def clean_phrase(p):\n",
    "    # keep alphabetic + hyphen; remove repeated spaces; drop very short tokens\n",
    "    p = re.sub(r\"[^a-z\\s\\-]\", \" \", p.lower())\n",
    "    p = re.sub(r\"\\s+\", \" \", p).strip()\n",
    "    # drop phrases containing generic stopwords entirely\n",
    "    toks = p.split()\n",
    "    if any(tok in GENERIC_STOP for tok in toks):\n",
    "        return None\n",
    "    # keep multiword phrases or domainy unigrams (litmus, neutralisation, indicator)\n",
    "    if len(toks)==1 and toks[0] not in {\"litmus\",\"indicator\",\"neutralisation\",\"acidic\",\"basic\",\"neutral\"}:\n",
    "        return None\n",
    "    # avoid phrases starting with verbs like \"let\", \"make\", \"see\"\n",
    "    if toks and toks[0] in {\"let\",\"make\",\"see\",\"find\",\"observe\",\"discuss\",\"try\"}:\n",
    "        return None\n",
    "    # min length\n",
    "    if len(\" \".join(toks)) < 5:\n",
    "        return None\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def tfidf_phrases(ch_blobs: List[str], book_blobs: List[str], top_k=40):\n",
    "    # Chapter-vs-book TFIDF with 2-4 grams to prefer phrases\n",
    "    # Relax min_df and max_df to handle smaller chapter texts or more unique terms\n",
    "    vect = TfidfVectorizer(\n",
    "        ngram_range=(2,4),\n",
    "        min_df=0.01,  # Lowered from default 1 -> Adjusted again\n",
    "        max_df=0.99,  # Increased from default 0.9 -> Adjusted again\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z\\-]+\\b\"\n",
    "    )\n",
    "    corpus = [\"\\n\".join(ch_blobs)] + [\"\\n\".join(book_blobs)]  # doc0=chapter, doc1+=book\n",
    "    try:\n",
    "        X = vect.fit_transform(corpus)\n",
    "    except ValueError as e:\n",
    "        if \"After pruning, no terms remain\" in str(e):\n",
    "            print(f\"Warning: TF-IDF found no terms for chapter. Consider adjusting min_df/max_df further or check input text.\")\n",
    "            return [] # Return empty list instead of raising error\n",
    "        else:\n",
    "            raise # Re-raise other ValueErrors\n",
    "    ch_vec = X[0].toarray()[0]\n",
    "    feats = vect.get_feature_names_out()\n",
    "    pairs = [(feats[i], ch_vec[i]) for i in ch_vec.nonzero()[0]]\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    out=[]\n",
    "    for term, sc in pairs[:max(top_k, 60)]:\n",
    "        cp = clean_phrase(term)\n",
    "        if cp:\n",
    "            out.append((cp, float(sc)))\n",
    "    return out\n",
    "\n",
    "def pages_for_phrase(phrase: str, blobs: List[str]):\n",
    "    hits=[]\n",
    "    pat = re.compile(r\"\\b\" + re.escape(phrase) + r\"\\b\", re.I)\n",
    "    for i, t in enumerate(blobs):\n",
    "        if pat.search(t):\n",
    "            hits.append(i)\n",
    "    return hits\n",
    "\n",
    "def robust_tfidf_terms(ch_blobs: List[str], book_blobs: List[str], want=60):\n",
    "    corpus = [\"\\n\".join(ch_blobs)] + [\"\\n\".join(book_blobs)]\n",
    "    attempts = [\n",
    "        dict(ngram_range=(2,4), stop_words=\"english\", max_df=0.95, min_df=1, token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z\\-]+\\b\"),\n",
    "        dict(ngram_range=(1,3), stop_words=None,      max_df=1.0,  min_df=1, token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z\\-]+\\b\"),\n",
    "        dict(ngram_range=(1,3), stop_words=None,      max_df=1.0,  min_df=1, token_pattern=r\"(?u)\\b\\w[\\w\\-]+\\b\"),\n",
    "    ]\n",
    "    for cfg in attempts:\n",
    "        try:\n",
    "            vect = TfidfVectorizer(**cfg)\n",
    "            X = vect.fit_transform(corpus)\n",
    "            if X.shape[1] == 0:\n",
    "                continue\n",
    "            ch_vec = X[0].toarray()[0]\n",
    "            feats = vect.get_feature_names_out()\n",
    "            # keep top features present in chapter doc\n",
    "            idxs = ch_vec.nonzero()[0]\n",
    "            pairs = [(feats[i], ch_vec[i]) for i in idxs]\n",
    "            pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "            out=[]\n",
    "            for term, sc in pairs[:max(want, 40)]:\n",
    "                cp = clean_phrase(term)\n",
    "                if cp:\n",
    "                    out.append((cp, float(sc)))\n",
    "            if out:\n",
    "                return out\n",
    "        except ValueError:\n",
    "            continue\n",
    "    # if everything fails, return empty and let cue/heading logic handle it\n",
    "    return []\n",
    "\n",
    "def merge_scores(tfidf_terms, boost_counter, boost_pages, blobs):\n",
    "    cand = {}\n",
    "    for p, s in tfidf_terms:\n",
    "        cand[p] = {\"score\": s, \"blob_indices\": pages_for_phrase(p, blobs)}\n",
    "\n",
    "    for raw, b in boost_counter.items():\n",
    "        p = clean_phrase(raw)\n",
    "        if not p: continue\n",
    "        hits = boost_pages.get(raw, set())\n",
    "        hits = [h for h in hits if h < len(blobs)]\n",
    "        c = cand.setdefault(p, {\"score\": 0.0, \"blob_indices\": hits})\n",
    "        c[\"score\"] += float(b) * 0.2\n",
    "        c[\"blob_indices\"] = sorted(set(c[\"blob_indices\"]) | set(hits))\n",
    "\n",
    "    for p, c in cand.items():\n",
    "        cov = len(set(c[\"blob_indices\"]))\n",
    "        c[\"score\"] *= (1.0 + 0.05 * cov)\n",
    "    return cand\n",
    "\n",
    "def pick_topics(cand: Dict[str, Dict[str, Any]], blobs: List[str], top_n: int):\n",
    "    items = sorted(cand.items(), key=lambda kv: kv[1][\"score\"], reverse=True)\n",
    "    picked=[]\n",
    "    for term, info in items:\n",
    "        if any(term in t for t,_ in picked if len(t) > len(term)+2):\n",
    "            continue\n",
    "        picked.append((term, info))\n",
    "        if len(picked) >= top_n*2:\n",
    "            break\n",
    "    final=[]\n",
    "    for term, info in picked[:top_n]:\n",
    "        idxs = info[\"blob_indices\"]\n",
    "        words = sum(len(blobs[i].split()) for i in idxs if i < len(blobs))\n",
    "        est = max(6, math.ceil(words/260.0) + 3)\n",
    "        final.append({\n",
    "            \"topic\": \" \".join(w.capitalize() if i==0 else w for i,w in enumerate(term.split())),\n",
    "            \"score\": round(info[\"score\"], 4),\n",
    "            \"blob_indices\": idxs,\n",
    "            \"estimated_minutes\": int(est)\n",
    "        })\n",
    "    return final\n",
    "\n",
    "\n",
    "def _llm_explain(topics, subject, grade, book, chapter):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    except Exception:\n",
    "        return topics\n",
    "    if not topics:\n",
    "        return topics\n",
    "    bullets = \"\\n\".join(f\"- {t['topic']}\" for t in topics)\n",
    "    system = (\"You help a student prioritize a chapter. For each topic name, \"\n",
    "              \"write ONE concise sentence on why it's important within the chapter scope. \"\n",
    "              \"Return JSON list of {topic, why_important}. No new topics.\")\n",
    "    user = f\"Subject: {subject}, Grade: {grade}, Book: {book}\\nChapter: {chapter}\\nTopics:\\n{bullets}\\n\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "            temperature=0\n",
    "        )\n",
    "        data = json.loads(resp.choices[0].message.content)\n",
    "        mp = {d[\"topic\"].strip().lower(): d[\"why_important\"] for d in data if \"topic\" in d and \"why_important\" in d}\n",
    "        for t in topics:\n",
    "            k = t[\"topic\"].lower()\n",
    "            if k in mp:\n",
    "                t[\"why_important\"] = mp[k][:240]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1758012291784,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "Taf6F0Wzz66O",
    "outputId": "6633c3d7-58e1-4afe-9def-d6a48f094ad0"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn, json, threading, time, os\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI, HTTPException, Query, Path\n",
    "\n",
    "app = FastAPI(title=\"NCERT Tutor API\")\n",
    "\n",
    "class ChatReq(BaseModel):\n",
    "    question: str\n",
    "    subject: str | None = None\n",
    "    grade: int | None = None\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(req: ChatReq):\n",
    "    ans, cites, subj, additional = chat_router(req.question, subject=req.subject, grade=req.grade)\n",
    "    return {\"subject_detected\": subj, \"answer\": ans, \"citations\": cites, \"more info\": additional}\n",
    "\n",
    "class PlanReq(BaseModel):\n",
    "    start: str\n",
    "    deadline: str\n",
    "    hours_per_day: float\n",
    "    subject: str | None = None\n",
    "    grade: int | None = None\n",
    "\n",
    "class ImportantTopicsReq(BaseModel):\n",
    "    subject: str\n",
    "    grade: int\n",
    "    book: str\n",
    "    chapter: str\n",
    "    top_n: int = 8\n",
    "    explain: bool = False  # if True, add one-line 'why_important' via LLM\n",
    "\n",
    "@app.post(\"/plan\")\n",
    "def plan_api(req: PlanReq):\n",
    "    rows = [json.loads(l) for l in open(\"/content/kb/meta.jsonl\", encoding=\"utf-8\")]\n",
    "    import math, collections\n",
    "    from datetime import date, timedelta\n",
    "\n",
    "    def estimate_minutes(txt):\n",
    "        tokens = max(50, len(txt.split()))\n",
    "        return max(6, math.ceil(tokens/40.0) + 2)\n",
    "\n",
    "    agg = {}\n",
    "    for r in rows:\n",
    "        if req.subject and (r.get(\"subject\") != req.subject):\n",
    "            continue\n",
    "        if req.grade and (r.get(\"grade\") != req.grade):\n",
    "            continue\n",
    "        key = (r[\"book\"], r.get(\"chapter\",\"\"), r.get(\"section\",\"\"))\n",
    "        a = agg.setdefault(key, {\"book\": r[\"book\"], \"chapter\": r.get(\"chapter\",\"\"), \"section\": r.get(\"section\",\"\"), \"pages\": set(), \"minutes_est\":0})\n",
    "        a[\"pages\"].add(r.get(\"page\",0))\n",
    "        a[\"minutes_est\"] += estimate_minutes(r[\"text\"])\n",
    "\n",
    "    sections = []\n",
    "    for (book,chap,sec), v in agg.items():\n",
    "        ps = sorted(v[\"pages\"])\n",
    "        v[\"pages\"] = f\"{ps[0]}-{ps[-1]}\" if ps else \"\"\n",
    "        v[\"id\"] = f\"{book}::{chap}::{sec}\"\n",
    "        sections.append(v)\n",
    "    sections = sorted(sections, key=lambda x: (x[\"book\"], x[\"chapter\"], x[\"section\"]))\n",
    "\n",
    "    start_d = date.fromisoformat(req.start)\n",
    "    end_d = date.fromisoformat(req.deadline)\n",
    "    minutes_per_day = int(req.hours_per_day*60)\n",
    "\n",
    "    days = []\n",
    "    d = start_d\n",
    "    while d <= end_d:\n",
    "        days.append({\"date\": d.isoformat(), \"capacity\": minutes_per_day, \"items\": []})\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "    i = 0\n",
    "    for sec in sections:\n",
    "        remaining = sec[\"minutes_est\"]\n",
    "        while remaining > 0 and i < len(days):\n",
    "            slot = min(remaining, days[i][\"capacity\"])\n",
    "            if slot == 0:\n",
    "                i += 1; continue\n",
    "            days[i][\"items\"].append({\"section_id\": sec[\"id\"], \"book\": sec[\"book\"], \"chapter\": sec[\"chapter\"], \"minutes\": slot})\n",
    "            days[i][\"capacity\"] -= slot\n",
    "            remaining -= slot\n",
    "\n",
    "    reviews = []\n",
    "    for day in days:\n",
    "        for item in day[\"items\"]:\n",
    "            for gap in (1,3,7):\n",
    "                try_date = (date.fromisoformat(day[\"date\"]) + timedelta(days=gap)).isoformat()\n",
    "                reviews.append({\"date\": try_date, \"section_id\": item[\"section_id\"], \"minutes\": max(6, item[\"minutes\"]//6)})\n",
    "\n",
    "    days_out = []\n",
    "    for d in days:\n",
    "        used = minutes_per_day - d[\"capacity\"]\n",
    "        days_out.append({\"date\": d[\"date\"], \"used_minutes\": used, \"items\": d[\"items\"]})\n",
    "\n",
    "    return {\"subject\": req.subject, \"grade\": req.grade, \"days\": days_out, \"reviews\": reviews}\n",
    "\n",
    "@app.get(\"/subjects\")\n",
    "def subjects_api():\n",
    "    with open(\"/content/kb/subjects.json\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@app.get(\"/topics\")\n",
    "def topics_api():\n",
    "    with open(\"/content/kb/topics.json\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@app.post(\"/important-topics\")\n",
    "def important_topics_api(req: ImportantTopicsReq):\n",
    "    rows = get_meta_rows()\n",
    "    ch_blobs, ch_pages, book_blobs = extract_chapter_and_book(rows, req.subject, req.grade, req.book, req.chapter)\n",
    "    if not ch_blobs:\n",
    "        raise HTTPException(status_code=404, detail=\"No matching chapter content found. Check subject/grade/book/chapter values.\")\n",
    "\n",
    "    tfidf_terms = robust_tfidf_terms(ch_blobs, book_blobs, want=max(60, req.top_n*8))\n",
    "    boosts, boost_pages = cue_heading_boosts(ch_blobs)\n",
    "\n",
    "    # If TF-IDF still gave nothing, fall back entirely to boosts\n",
    "    if not tfidf_terms:\n",
    "        cand = {}\n",
    "        for raw, b in boosts.items():\n",
    "            p = clean_phrase(raw)\n",
    "            if not p: continue\n",
    "            hits = [h for h in boost_pages.get(raw, set()) if h < len(ch_blobs)]\n",
    "            cand[p] = {\"score\": float(b), \"blob_indices\": hits}\n",
    "    else:\n",
    "        cand = merge_scores(tfidf_terms, boosts, boost_pages, ch_blobs)\n",
    "\n",
    "    topics = pick_topics(cand, ch_blobs, req.top_n)\n",
    "\n",
    "    # map to real pages\n",
    "    for t in topics:\n",
    "        idxs = t.pop(\"blob_indices\", [])\n",
    "        pages = [ch_pages[i] for i in idxs if i < len(ch_pages)]\n",
    "        t[\"pages\"] = compress_ranges(pages)\n",
    "        t[\"why_important\"] = t.get(\"why_important\") or None\n",
    "\n",
    "    if req.explain:\n",
    "        topics = _llm_explain(topics, req.subject, req.grade, req.book, req.chapter)\n",
    "\n",
    "    return {\n",
    "        \"subject\": req.subject,\n",
    "        \"grade\": req.grade,\n",
    "        \"book\": req.book,\n",
    "        \"chapter\": req.chapter,\n",
    "        \"method\": \"robust-tfidf+cue-fallback\" + (\"+llm\" if req.explain else \"\"),\n",
    "        \"topics\": topics\n",
    "    }\n",
    "\n",
    "if os.path.exists(PROGRESS_JSON):\n",
    "    with open(PROGRESS_JSON, encoding=\"utf-8\") as f:\n",
    "        progress_store = json.load(f)\n",
    "else:\n",
    "    progress_store = {\"completed\": []}\n",
    "\n",
    "class ProgressUpdate(BaseModel):\n",
    "    user_id: str\n",
    "    section_id: str\n",
    "    completed_on: str  # ISO date\n",
    "\n",
    "@app.post(\"/progress\")\n",
    "def progress_update(p: ProgressUpdate):\n",
    "    progress_store[\"completed\"].append(p.model_dump())\n",
    "    with open(PROGRESS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(progress_store, f, ensure_ascii=False, indent=2)\n",
    "    return {\"ok\": True, \"completed_count\": len(progress_store[\"completed\"])}\n",
    "\n",
    "def run_server():\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "print(\"FastAPI started on http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1758012297591,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "O8UW6ztX07Jy",
    "outputId": "3839b6d5-6011-442f-c13a-8044f8b4e1b5"
   },
   "outputs": [],
   "source": [
    "import requests, json, os\n",
    "base = \"http://127.0.0.1:8000\"\n",
    "r = requests.get(base + \"/topics\")\n",
    "print(\"Subjects found:\", len(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10284,
     "status": "ok",
     "timestamp": 1758012068405,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "rGXqPreN1PIC",
    "outputId": "8f606fb6-dc99-4634-9ac9-e5bbffc21b45"
   },
   "outputs": [],
   "source": [
    "r = requests.post(base + \"/chat\", json={\"question\":\"What is Myopia?\"})\n",
    "print(json.dumps(r.json(), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8370,
     "status": "ok",
     "timestamp": 1758012310527,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "v-FaNSxa6pZu",
    "outputId": "412c88f9-e6fd-4538-881b-1f75924f383c"
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "r = requests.post(base + \"/important-topics\", json={\n",
    "    \"subject\": \"science\",\n",
    "    \"grade\": 10,\n",
    "    \"book\": \"science_grade10_10\",          # Corrected book name to match meta.jsonl\n",
    "    \"chapter\": \"10\",  # exact chapter string from meta\n",
    "    \"top_n\": 8,\n",
    "    \"explain\": True\n",
    "})\n",
    "print(json.dumps(r.json(), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1758012075336,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "8NEf9tm8UnFL",
    "outputId": "b2cd53db-3836-43d3-cf0f-2c98ac35f4c2"
   },
   "outputs": [],
   "source": [
    "base = \"http://127.0.0.1:8000\"\n",
    "r = requests.get(base + \"/topics\")\n",
    "print(\"Topics found:\", r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1758012075537,
     "user": {
      "displayName": "Sutanu D",
      "userId": "15613842179717920405"
     },
     "user_tz": -330
    },
    "id": "yZkFThxv18ud",
    "outputId": "5461987c-41e7-48c1-f97a-9a50aa9894aa"
   },
   "outputs": [],
   "source": [
    "r = requests.post(base + \"/plan\", json={\"start\":\"2025-09-16\",\"deadline\":\"2025-10-01\",\"hours_per_day\":1.5})\n",
    "print(\"Plan keys:\", r.json())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMuZFZUQ+Jv8KSwHFGzPqlL",
   "provenance": [
    {
     "file_id": "15x-PwpQJcyIOr1KyvdRzF5g2jQ61ytU0",
     "timestamp": 1758012639781
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
